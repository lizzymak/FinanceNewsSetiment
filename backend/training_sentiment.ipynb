{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571d32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7973cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "df = pd.read_csv(\"news.csv\", encoding='latin-1', header=None) \n",
    "df.columns = ['label', 'text']\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb208ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert string to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove links\n",
    "    text = re.sub(r'\\@w+|\\#','', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove numbers and punctuation\n",
    "    return text\n",
    "df['text'] = df['text'].astype(str).apply(clean_text) # updates text column to string and cleans\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])  \n",
    "\n",
    "print(\"Label classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea762b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/tests split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "# here were training it so it can predict what label each text goes under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfd2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4039\n"
     ]
    }
   ],
   "source": [
    "# create data set class\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# splits text at any whitespace\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# goes through all of the text and yeilds the tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenizer(text))\n",
    "    # start indices at 2 so we can reserve 0 for PAD and 1 for UNK\n",
    "    vocab = {word: i+2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_dict = build_vocab(train_texts, min_freq=2)  \n",
    "print(\"Vocab size:\", len(vocab_dict))\n",
    "\n",
    "def numericalize(text, vocab):\n",
    "    tokens = tokenizer(text)  # split into words\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4489b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def load_glove_embeddings(glove_file_path, vocab, embedding_dim = 50):\n",
    "#     embeddings_index = {}\n",
    "#     with open(glove_file_path, encoding=\"utf8\") as f:\n",
    "#         for line in f:\n",
    "#             values = line.split() #split at every space\n",
    "#             word = values[0] #the word is the first thing in each line\n",
    "#             vector = np.asarray(values[1:], dtype=\"float32\") #convert rest of inputs in line to numpy array\n",
    "#             embeddings_index[word] = vector\n",
    "    \n",
    "#     embedding_matrix = np.zeros((max(vocab.values()) + 1, embedding_dim)) #nump array filled with 0's initially and has a dimention of 50 for each word\n",
    "#     # function adds word's vector into our matrix\n",
    "#     for word, idx in vocab.items():\n",
    "#         vector = embeddings_index.get(word)\n",
    "#         if vector is not None:\n",
    "#             embedding_matrix[idx] = vector\n",
    "#         else:\n",
    "#             embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "#     return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# embedding_matrix = load_glove_embeddings(\"glove.6B.50d.txt\", vocab_dict, embedding_dim=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8159b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 122\n",
      "Test batches: 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    # number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # call when we want only one sample\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        tokens = torch.tensor(numericalize(text, self.vocab), dtype=torch.long) #self.vocab is for mapping words to numbers\n",
    "        return tokens, torch.tensor(label, dtype=torch.long) #returns tokens and labels\n",
    "    \n",
    "# make data into dataset object\n",
    "train_dataset = NewsDataset(train_texts, train_labels, vocab_dict)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, vocab_dict)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0) #makes all lists same length by adding zeros at end\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# train 32 healines per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch) # reshuffle every epoch\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd78dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  #turns vocab id into dense vector\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes) # input is average embedding of a sentence and output is num_classes(2)cwhich is buy or sell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (batch size, seq len)\n",
    "        embedded = self.embedding(x) # gives us original output and embed_dim\n",
    "        pooling = embedded.mean(dim = 1) # average embedding across all words in teh sentence, used to get one vec per sentence\n",
    "        output = self.fc(pooling) # passes vector into classifier\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3712cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, loss, optimizer\n",
    "vocab_size = max(vocab_dict.values()) + 1 # amount of unique words\n",
    "embed_dim = 50 # size of each word vector, the larger the number the more expressive the word is\n",
    "num_classes = len(set(train_labels)) # number of classes -> 2(buy/sell)\n",
    "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss() # diff between prediction and target\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # optimizer that updates weights using gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7c5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training loop\n",
    "def train_model(self, train_loader, criterion, optimizer, epochs = 5):\n",
    "    model.train() # put in training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for texts, label in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            outputs = model(texts) # forward pass\n",
    "            loss = criterion(outputs, label) # calculate error\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update weights\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c63019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, test_loader):\n",
    "    model.eval() # put in evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(): # gradients not needed in eval mode\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts) # forward pass\n",
    "            _, predicted = torch.max(outputs, 1) # get prediction index 0 = sell 1 = buy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f34b1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 27.8095\n",
      "Test Accuracy: 77.01%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea36491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model\n",
    "import torch.nn.functional as F\n",
    "def prediction(text, model, vocab, max_len = 50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tokenize\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = [vocab_dict.get(word, vocab[\"<unk>\"]) for word in tokens]\n",
    "\n",
    "        # pad\n",
    "        if len(tokens) < max_len:\n",
    "            token_ids += [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "\n",
    "        input_tensor = torch.tensor([token_ids])\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        probs = F.softmax(output, dim=1) # rescales input and has it sum to 1, probability distribution\n",
    "        predicted_class = torch.argmax(probs, dim=1).item() # picks class with highest probability\n",
    "        return predicted_class, probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b4509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, array([[0.26972422, 0.33207425, 0.3982015 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Recreate vocab + model\n",
    "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "model.load_state_dict(torch.load(\"sentiment_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Then predict\n",
    "sentiment = prediction(\"Tesla stock surges after earnings\", model, vocab_dict)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a81216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n",
      "Probabilities: [[0.26972422 0.33207425 0.3982015 ]]\n",
      "Text: Stock prices soared after the company announced record profits\n",
      "Predicted Class: 1, Probabilities: [[0.18211912 0.5038501  0.31403086]]\n",
      "\n",
      "Text: The market crashed due to unexpected inflation data\n",
      "Predicted Class: 1, Probabilities: [[0.10302711 0.8363166  0.06065637]]\n",
      "\n",
      "Text: Investors are optimistic about new technology developments\n",
      "Predicted Class: 1, Probabilities: [[0.0652108  0.70962    0.22516921]]\n",
      "\n",
      "Text: Stock prices soar after company reports record earnings\n",
      "Predicted Class: 2, Probabilities: [[0.26972422 0.33207425 0.3982015 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Stock prices soar after company reports record earnings\"\n",
    "pred_class, pred_probs = prediction(sample_text, model, vocab_dict)\n",
    "\n",
    "print(\"Prediction:\", pred_class)\n",
    "print(\"Probabilities:\", pred_probs)\n",
    "example_texts = [\n",
    "    \"Stock prices soared after the company announced record profits\",\n",
    "    \"The market crashed due to unexpected inflation data\",\n",
    "    \"Investors are optimistic about new technology developments\",\n",
    "    \"Stock prices soar after company reports record earnings\",\n",
    "    \n",
    "]\n",
    "\n",
    "for txt in example_texts:\n",
    "    pred_class, probs = prediction(txt, model, vocab_dict)\n",
    "    print(f\"Text: {txt}\")\n",
    "    print(f\"Predicted Class: {pred_class}, Probabilities: {probs}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da35a8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pred, probs\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Use it:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m debug_predict(\u001b[33m\"\u001b[39m\u001b[33mThe market crashed due to unexpected inflation data\u001b[39m\u001b[33m\"\u001b[39m, model, vocab_dict, \u001b[43mlabel_map\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'label_map' is not defined"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F, numpy as np\n",
    "\n",
    "def debug_predict(text, model, vocab, label_map, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(w, vocab.get(\"<unk>\", 1)) for w in tokens]\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    unk_count = sum(1 for w in tokens if w not in vocab)\n",
    "    print(f\"UNKs: {unk_count}/{len(tokens)} ({unk_count/len(tokens):.2%})\")\n",
    "\n",
    "    # pad/truncate\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab.get(\"<pad>\", 0)] * (max_len - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_len]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(torch.tensor([token_ids]))\n",
    "        probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "        pred = int(np.argmax(probs))\n",
    "\n",
    "    print(\"Predicted index:\", pred, \"Label:\", label_map[pred])\n",
    "    print(\"Probabilities:\", {label_map[i]: float(probs[i]) for i in range(len(probs))})\n",
    "    return pred, probs\n",
    "\n",
    "# Use it:\n",
    "#debug_predict(\"The market crashed due to unexpected inflation data\", model, vocab_dict, label_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
