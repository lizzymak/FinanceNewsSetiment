{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "571d32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7973cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "df = pd.read_csv(\"news.csv\", encoding='latin-1', header=None) \n",
    "df.columns = ['label', 'text']\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfb208ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert string to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove links\n",
    "    text = re.sub(r'\\@w+|\\#','', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove numbers and punctuation\n",
    "    return text\n",
    "df['text'] = df['text'].astype(str).apply(clean_text) # updates text column to string and cleans\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])  \n",
    "\n",
    "print(\"Label classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aea762b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/tests split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "# here were training it so it can predict what label each text goes under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dfd2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4039\n"
     ]
    }
   ],
   "source": [
    "# create data set class\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# splits text at any whitespace\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# goes through all of the text and yeilds the tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenizer(text))\n",
    "    # start indices at 2 so we can reserve 0 for PAD and 1 for UNK\n",
    "    vocab = {word: i+2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_dict = build_vocab(train_texts, min_freq=2)  \n",
    "print(\"Vocab size:\", len(vocab_dict))\n",
    "\n",
    "def numericalize(text, vocab):\n",
    "    tokens = tokenizer(text)  # split into words\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8159b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 122\n",
      "Test batches: 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    # number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # call when we want only one sample\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        tokens = torch.tensor(numericalize(text, self.vocab), dtype=torch.long) #self.vocab is for mapping words to numbers\n",
    "        return tokens, torch.tensor(label, dtype=torch.long) #returns tokens and labels\n",
    "    \n",
    "# make data into dataset object\n",
    "train_dataset = NewsDataset(train_texts, train_labels, vocab_dict)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, vocab_dict)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0) #makes all lists same length by adding zeros at end\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# train 32 healines per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch) # reshuffle every epoch\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd78dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # turns vocab id into dense vector\n",
    "        self.fc = nn.Linear(embed_dim, num_classes) # input is average embedding of a sentence and output is num_classes(2)cwhich is buy or sell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (batch size, seq len)\n",
    "        embedded = self.embedding(x) # gives us original output and embed_dim\n",
    "        pooling = embedded.mean(dim = 1) # average embedding across all words in teh sentence, used to get one vec per sentence\n",
    "        output = self.fc(pooling) # passes vector into classifier\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3712cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, loss, optimizer\n",
    "vocab_size = max(vocab_dict.values()) + 1 # amount of unique words\n",
    "embed_dim = 50 # size of each word vector, the larger the number the more expressive the word is\n",
    "num_classes = len(set(train_labels)) # number of classes -> 2(buy/sell)\n",
    "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss() # diff between prediction and target\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # optimizer that updates weights using gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a7c5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training loop\n",
    "def train_model(self, train_loader, criterion, optimizer, epochs = 5):\n",
    "    model.train() # put in training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for texts, label in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            outputs = model(texts) # forward pass\n",
    "            loss = criterion(outputs, label) # calculate error\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update weights\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02c63019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, test_loader):\n",
    "    model.eval() # put in evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(): # gradients not needed in eval mode\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts) # forward pass\n",
    "            _, predicted = torch.max(outputs, 1) # get prediction index 0 = sell 1 = buy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f34b1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 74.0861\n",
      "Epoch 2/10, Loss: 71.8376\n",
      "Epoch 3/10, Loss: 68.4318\n",
      "Epoch 4/10, Loss: 66.1710\n",
      "Epoch 5/10, Loss: 62.4513\n",
      "Epoch 6/10, Loss: 60.1393\n",
      "Epoch 7/10, Loss: 57.3580\n",
      "Epoch 8/10, Loss: 55.3067\n",
      "Epoch 9/10, Loss: 52.5003\n",
      "Epoch 10/10, Loss: 50.0892\n",
      "Test Accuracy: 75.46%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model\n",
    "def prediction(text, model, vocab, max_len = 50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tokenize\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = [vocab_dict.get(word, vocab[\"<unk\"]) for word in tokens]\n",
    "\n",
    "        # pad\n",
    "        if len(tokens) < max_len:\n",
    "            token_ids += [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "\n",
    "        input_tensor = torch.tensor([token_ids])\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        probs = F.softmax(output, dim=1) # rescales input and has it sum to 1, probability distribution\n",
    "        predicted_class = torch.argmax(probs, dim=1).item() # picks class with highest probability\n",
    "        return predicted_class, probs.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
