{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7968258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaa37f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "df = pd.read_csv(\"data.csv\", encoding='latin-1') \n",
    "df.columns = ['label', 'text']\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "671ce278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert string to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove links\n",
    "    text = re.sub(r'\\@w+|\\#','', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove numbers and punctuation\n",
    "    return text\n",
    "df['text'] = df['text'].astype(str).apply(clean_text) # updates text column to string and cleans\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])  \n",
    "\n",
    "print(\"Label classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae1620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# train/tests split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "# here were training it so it can predict what label each text goes under\n",
    "print(\"Unique labels:\", sorted(set(train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ff115e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set class\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "# splits text at any whitespace\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# goes through all of the text and yeilds the tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenizer(text))\n",
    "    # start indices at 2 so we can reserve 0 for PAD and 1 for UNK\n",
    "    vocab = {word: i+2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_dict = build_vocab(train_texts, min_freq=2)  \n",
    "\n",
    "words_to_keep = list(vocab_dict.keys())\n",
    "\n",
    "# Create a new dictionary assigning completely new sequential indices\n",
    "new_vocab_dict = {}\n",
    "for i, word in enumerate(words_to_keep):\n",
    "    if word not in [\"<pad>\", \"<unk>\"]:\n",
    "        new_vocab_dict[word] = i + 2\n",
    "\n",
    "new_vocab_dict[\"<pad>\"] = 0\n",
    "new_vocab_dict[\"<unk>\"] = 1\n",
    "vocab_dict = new_vocab_dict\n",
    "final_length = len(vocab_dict)\n",
    "final_max_index = max(vocab_dict.values())\n",
    "\n",
    "\n",
    "def numericalize(text, vocab):\n",
    "    tokens = tokenizer(text)  # split into words\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "548ab771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 122\n",
      "Test batches: 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    # number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # call when we want only one sample\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        tokens = torch.tensor(numericalize(text, self.vocab), dtype=torch.long) #self.vocab is for mapping words to numbers\n",
    "        return tokens, torch.tensor(label, dtype=torch.long) #returns tokens and labels\n",
    "    \n",
    "# make data into dataset object\n",
    "train_dataset = NewsDataset(train_texts, train_labels, vocab_dict)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, vocab_dict)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0) #makes all lists same length by adding zeros at end\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# train 32 healines per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch) # reshuffle every epoch\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "837d7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  #turns vocab id into dense vector\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes) # input is average embedding of a sentence and output is num_classes(2)cwhich is buy or sell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (batch size, seq len)\n",
    "        embedded = self.embedding(x) # gives us original output and embed_dim\n",
    "        pooling = embedded.mean(dim = 1) # average embedding across all words in teh sentence, used to get one vec per sentence\n",
    "        output = self.fc(pooling) # passes vector into classifier\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17c0c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, loss, optimizer\n",
    "vocab_size = final_max_index + 1 # amount of unique words\n",
    "embed_dim = 50 # size of each word vector, the larger the number the more expressive the word is\n",
    "num_classes = len(set(train_labels)) # number of classes -> 2(buy/sell)\n",
    "\n",
    "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # diff between prediction and target\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # optimizer that updates weights using gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d2c3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training loop\n",
    "def train_model(self, train_loader, criterion, optimizer, epochs = 5):\n",
    "    model.train() # put in training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for texts, label in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            outputs = model(texts) # forward pass\n",
    "            loss = criterion(outputs, label) # calculate error\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update weights\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d0f4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, test_loader):\n",
    "    model.eval() # put in evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(): # gradients not needed in eval mode\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts) # forward pass\n",
    "            _, predicted = torch.max(outputs, 1) # get prediction index 0 = sell 1 = buy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a08bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 115.0568\n",
      "Epoch 2/30, Loss: 109.9712\n",
      "Epoch 3/30, Loss: 105.8915\n",
      "Epoch 4/30, Loss: 102.7013\n",
      "Epoch 5/30, Loss: 98.5057\n",
      "Epoch 6/30, Loss: 94.2129\n",
      "Epoch 7/30, Loss: 90.7214\n",
      "Epoch 8/30, Loss: 87.1971\n",
      "Epoch 9/30, Loss: 83.9083\n",
      "Epoch 10/30, Loss: 81.2496\n",
      "Epoch 11/30, Loss: 78.0844\n",
      "Epoch 12/30, Loss: 74.4518\n",
      "Epoch 13/30, Loss: 71.4954\n",
      "Epoch 14/30, Loss: 68.3382\n",
      "Epoch 15/30, Loss: 65.4013\n",
      "Epoch 16/30, Loss: 62.4374\n",
      "Epoch 17/30, Loss: 59.6437\n",
      "Epoch 18/30, Loss: 56.9152\n",
      "Epoch 19/30, Loss: 54.2372\n",
      "Epoch 20/30, Loss: 52.6700\n",
      "Epoch 21/30, Loss: 49.8093\n",
      "Epoch 22/30, Loss: 47.4438\n",
      "Epoch 23/30, Loss: 45.2833\n",
      "Epoch 24/30, Loss: 42.9911\n",
      "Epoch 25/30, Loss: 42.0814\n",
      "Epoch 26/30, Loss: 39.3460\n",
      "Epoch 27/30, Loss: 37.7534\n",
      "Epoch 28/30, Loss: 36.4365\n",
      "Epoch 29/30, Loss: 34.6813\n",
      "Epoch 30/30, Loss: 33.1965\n",
      "Test Accuracy: 77.22%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=30)\n",
    "\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1de7f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([4039, 50])\n",
      "fc.weight torch.Size([3, 50])\n",
      "fc.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"sentiment_model.pth\", map_location=\"cpu\")\n",
    "for key, value in checkpoint.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b7f1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model\n",
    "import torch.nn.functional as F\n",
    "def prediction(text, model, vocab, max_len = 50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tokenize\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = [vocab_dict.get(word, vocab[\"<unk>\"]) for word in tokens]\n",
    "\n",
    "        # pad\n",
    "        if len(tokens) < max_len:\n",
    "            token_ids += [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "\n",
    "        input_tensor = torch.tensor([token_ids])\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        probs = F.softmax(output, dim=1) # rescales input and has it sum to 1, probability distribution\n",
    "        predicted_class = torch.argmax(probs, dim=1).item() # picks class with highest probability\n",
    "        return predicted_class, probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa898b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n",
      "Probabilities: [[0.1959698 0.2850381 0.5189921]]\n",
      "Text: With the new production plant the company would increase its capacity to meet the expected increase\n",
      "Predicted Class: 2, Probabilities: [[0.00738128 0.12397449 0.86864424]]\n",
      "\n",
      "Text: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\n",
      "Predicted Class: 2, Probabilities: [[0.1801121  0.03482138 0.7850665 ]]\n",
      "\n",
      "Text: The international electronic industry company Elcoteq has laid off tens of employees from its Tallin\n",
      "Predicted Class: 0, Probabilities: [[0.5691825  0.09639616 0.3344213 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Stock prices soar after company reports record earnings\"\n",
    "pred_class, pred_probs = prediction(sample_text, model, vocab_dict)\n",
    "\n",
    "print(\"Prediction:\", pred_class)\n",
    "print(\"Probabilities:\", pred_probs)\n",
    "example_texts = [\n",
    "    \"With the new production plant the company would increase its capacity to meet the expected increase\",\n",
    "    \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\",\n",
    "    \"The international electronic industry company Elcoteq has laid off tens of employees from its Tallin\"\n",
    "]\n",
    "\n",
    "for txt in example_texts:\n",
    "    pred_class, probs = prediction(txt, model, vocab_dict)\n",
    "    print(f\"Text: {txt}\")\n",
    "    print(f\"Predicted Class: {pred_class}, Probabilities: {probs}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7297ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder classes (index -> label):\n",
      "0 negative\n",
      "1 neutral\n",
      "2 positive\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# If you used sklearn LabelEncoder earlier and saved it as `encoder`:\n",
    "print(\"Label encoder classes (index -> label):\")\n",
    "for i, lbl in enumerate(encoder.classes_):\n",
    "    print(i, lbl)\n",
    "\n",
    "# If you used manual mapping like {\"negative\":0, \"neutral\":1, \"positive\":2}:\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "print(label_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
