{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571d32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "Index(['label', 'text'], dtype='object')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "df = pd.read_csv(\"data.csv\", encoding='latin-1') \n",
    "df.columns = ['label', 'text']\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb208ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert string to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove links\n",
    "    text = re.sub(r'\\@w+|\\#','', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove numbers and punctuation\n",
    "    return text\n",
    "df['text'] = df['text'].astype(str).apply(clean_text) # updates text column to string and cleans\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])  \n",
    "\n",
    "print(\"Label classes:\", encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea762b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# train/tests split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "# here were training it so it can predict what label each text goes under\n",
    "print(\"Unique labels:\", sorted(set(train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfd2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4039\n"
     ]
    }
   ],
   "source": [
    "# create data set class\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "# splits text at any whitespace\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# goes through all of the text and yeilds the tokens\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenizer(text))\n",
    "    # start indices at 2 so we can reserve 0 for PAD and 1 for UNK\n",
    "    vocab = {word: i+2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab_dict = build_vocab(train_texts, min_freq=2)  \n",
    "print(\"Vocab size:\", len(vocab_dict))\n",
    "\n",
    "def numericalize(text, vocab):\n",
    "    tokens = tokenizer(text)  # split into words\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"vocab_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4489b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def load_glove_embeddings(glove_file_path, vocab, embedding_dim = 50):\n",
    "#     embeddings_index = {}\n",
    "#     with open(glove_file_path, encoding=\"utf8\") as f:\n",
    "#         for line in f:\n",
    "#             values = line.split() #split at every space\n",
    "#             word = values[0] #the word is the first thing in each line\n",
    "#             vector = np.asarray(values[1:], dtype=\"float32\") #convert rest of inputs in line to numpy array\n",
    "#             embeddings_index[word] = vector\n",
    "    \n",
    "#     embedding_matrix = np.zeros((max(vocab.values()) + 1, embedding_dim)) #nump array filled with 0's initially and has a dimention of 50 for each word\n",
    "#     # function adds word's vector into our matrix\n",
    "#     for word, idx in vocab.items():\n",
    "#         vector = embeddings_index.get(word)\n",
    "#         if vector is not None:\n",
    "#             embedding_matrix[idx] = vector\n",
    "#         else:\n",
    "#             embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "#     return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# embedding_matrix = load_glove_embeddings(\"glove.6B.50d.txt\", vocab_dict, embedding_dim=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8159b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 122\n",
      "Test batches: 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    # number of samples in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # call when we want only one sample\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        tokens = torch.tensor(numericalize(text, self.vocab), dtype=torch.long) #self.vocab is for mapping words to numbers\n",
    "        return tokens, torch.tensor(label, dtype=torch.long) #returns tokens and labels\n",
    "    \n",
    "# make data into dataset object\n",
    "train_dataset = NewsDataset(train_texts, train_labels, vocab_dict)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, vocab_dict)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0) #makes all lists same length by adding zeros at end\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# train 32 healines per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch) # reshuffle every epoch\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd78dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  #turns vocab id into dense vector\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes) # input is average embedding of a sentence and output is num_classes(2)cwhich is buy or sell\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (batch size, seq len)\n",
    "        embedded = self.embedding(x) # gives us original output and embed_dim\n",
    "        pooling = embedded.mean(dim = 1) # average embedding across all words in teh sentence, used to get one vec per sentence\n",
    "        output = self.fc(pooling) # passes vector into classifier\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, loss, optimizer\n",
    "vocab_size = max(vocab_dict.values()) + 1 # amount of unique words\n",
    "embed_dim = 50 # size of each word vector, the larger the number the more expressive the word is\n",
    "num_classes = len(set(train_labels)) # number of classes -> 2(buy/sell)\n",
    "model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss() # diff between prediction and target\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # optimizer that updates weights using gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7c5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training loop\n",
    "def train_model(self, train_loader, criterion, optimizer, epochs = 5):\n",
    "    model.train() # put in training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for texts, label in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients\n",
    "            outputs = model(texts) # forward pass\n",
    "            loss = criterion(outputs, label) # calculate error\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update weights\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c63019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(self, test_loader):\n",
    "    model.eval() # put in evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(): # gradients not needed in eval mode\n",
    "        for texts, labels in test_loader:\n",
    "            outputs = model(texts) # forward pass\n",
    "            _, predicted = torch.max(outputs, 1) # get prediction index 0 = sell 1 = buy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34b1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 114.2123\n",
      "Epoch 2/30, Loss: 110.9316\n",
      "Epoch 3/30, Loss: 107.5460\n",
      "Epoch 4/30, Loss: 103.7880\n",
      "Epoch 5/30, Loss: 100.0297\n",
      "Epoch 6/30, Loss: 96.4197\n",
      "Epoch 7/30, Loss: 92.7011\n",
      "Epoch 8/30, Loss: 88.9235\n",
      "Epoch 9/30, Loss: 85.2008\n",
      "Epoch 10/30, Loss: 81.9970\n",
      "Epoch 11/30, Loss: 78.3204\n",
      "Epoch 12/30, Loss: 75.2570\n",
      "Epoch 13/30, Loss: 72.4520\n",
      "Epoch 14/30, Loss: 69.0984\n",
      "Epoch 15/30, Loss: 65.8315\n",
      "Epoch 16/30, Loss: 63.0408\n",
      "Epoch 17/30, Loss: 59.9910\n",
      "Epoch 18/30, Loss: 57.7102\n",
      "Epoch 19/30, Loss: 54.6150\n",
      "Epoch 20/30, Loss: 52.2284\n",
      "Epoch 21/30, Loss: 50.3947\n",
      "Epoch 22/30, Loss: 47.7134\n",
      "Epoch 23/30, Loss: 45.6004\n",
      "Epoch 24/30, Loss: 43.5257\n",
      "Epoch 25/30, Loss: 41.5999\n",
      "Epoch 26/30, Loss: 40.1510\n",
      "Epoch 27/30, Loss: 38.5052\n",
      "Epoch 28/30, Loss: 36.9869\n",
      "Epoch 29/30, Loss: 35.6788\n",
      "Epoch 30/30, Loss: 33.9636\n",
      "embedding.weight torch.Size([8404, 50])\n",
      "fc.weight torch.Size([3, 50])\n",
      "fc.bias torch.Size([3])\n",
      "Test Accuracy: 75.67%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=30)\n",
    "\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "checkpoint = torch.load(\"sentiment_model.pth\", map_location=\"cpu\")\n",
    "for key, value in checkpoint.items():\n",
    "    print(key, value.shape)\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea36491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model\n",
    "import torch.nn.functional as F\n",
    "def prediction(text, model, vocab, max_len = 50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tokenize\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = [vocab_dict.get(word, vocab[\"<unk>\"]) for word in tokens]\n",
    "\n",
    "        # pad\n",
    "        if len(tokens) < max_len:\n",
    "            token_ids += [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "\n",
    "        input_tensor = torch.tensor([token_ids])\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        probs = F.softmax(output, dim=1) # rescales input and has it sum to 1, probability distribution\n",
    "        predicted_class = torch.argmax(probs, dim=1).item() # picks class with highest probability\n",
    "        return predicted_class, probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5b4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recreate vocab + model\n",
    "# model = TextClassifier(vocab_size, embed_dim, num_classes)\n",
    "# model.load_state_dict(torch.load(\"sentiment_model.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# # Then predict\n",
    "# sentiment = prediction(\"Tesla stock decreases alot. \", model, vocab_dict)\n",
    "# print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a81216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n",
      "Probabilities: [[0.16540395 0.41472107 0.41987497]]\n",
      "Text: With the new production plant the company would increase its capacity to meet the expected increase\n",
      "Predicted Class: 2, Probabilities: [[0.0046184  0.08148839 0.9138933 ]]\n",
      "\n",
      "Text: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\n",
      "Predicted Class: 2, Probabilities: [[0.12834062 0.0231664  0.848493  ]]\n",
      "\n",
      "Text: The international electronic industry company Elcoteq has laid off tens of employees from its Tallin\n",
      "Predicted Class: 0, Probabilities: [[0.69263464 0.12259816 0.18476723]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Stock prices soar after company reports record earnings\"\n",
    "pred_class, pred_probs = prediction(sample_text, model, vocab_dict)\n",
    "\n",
    "print(\"Prediction:\", pred_class)\n",
    "print(\"Probabilities:\", pred_probs)\n",
    "example_texts = [\n",
    "    \"With the new production plant the company would increase its capacity to meet the expected increase\",\n",
    "    \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing\",\n",
    "    \"The international electronic industry company Elcoteq has laid off tens of employees from its Tallin\"\n",
    "\n",
    "]\n",
    "\n",
    "for txt in example_texts:\n",
    "    pred_class, probs = prediction(txt, model, vocab_dict)\n",
    "    print(f\"Text: {txt}\")\n",
    "    print(f\"Predicted Class: {pred_class}, Probabilities: {probs}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cca29b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder classes (index -> label):\n",
      "0 negative\n",
      "1 neutral\n",
      "2 positive\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# If you used sklearn LabelEncoder earlier and saved it as `encoder`:\n",
    "print(\"Label encoder classes (index -> label):\")\n",
    "for i, lbl in enumerate(encoder.classes_):\n",
    "    print(i, lbl)\n",
    "\n",
    "# If you used manual mapping like {\"negative\":0, \"neutral\":1, \"positive\":2}:\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da35a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: buy\n",
      "Tokens: ['buy']\n",
      "Token IDs: [700]\n",
      "UNKs: 0/1 (0.00%)\n",
      "Predicted index: 1 Label: neutral\n",
      "Probabilities: {'negative': 0.06489437818527222, 'neutral': 0.7436578869819641, 'positive': 0.19144777953624725}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, array([0.06489438, 0.7436579 , 0.19144778], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F, numpy as np\n",
    "\n",
    "def debug_predict(text, model, vocab, label_map, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(w, vocab.get(\"<unk>\", 1)) for w in tokens]\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    unk_count = sum(1 for w in tokens if w not in vocab)\n",
    "    print(f\"UNKs: {unk_count}/{len(tokens)} ({unk_count/len(tokens):.2%})\")\n",
    "\n",
    "    # pad/truncate\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab.get(\"<pad>\", 0)] * (max_len - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_len]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(torch.tensor([token_ids]))\n",
    "        probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "        pred = int(np.argmax(probs))\n",
    "\n",
    "    print(\"Predicted index:\", pred, \"Label:\", label_map[pred])\n",
    "    print(\"Probabilities:\", {label_map[i]: float(probs[i]) for i in range(len(probs))})\n",
    "    return pred, probs\n",
    "\n",
    "# Use it:\n",
    "debug_predict(\"buy\", model, vocab_dict, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672af62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load your processed dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# has 'label' and 'text' columns\u001b[39;00m\n\u001b[32m      6\u001b[39m df.columns = [\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Optional: only keep positive/negative labels if you want\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lizma\\Documents\\GitHub\\StockPredictor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lizma\\Documents\\GitHub\\StockPredictor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lizma\\Documents\\GitHub\\StockPredictor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lizma\\Documents\\GitHub\\StockPredictor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lizma\\Documents\\GitHub\\StockPredictor\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'all-data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load your processed dataset\n",
    "df = pd.read_csv(\"data.csv\", encoding='latin-1')  # has 'label' and 'text' columns\n",
    "df.columns = ['label', 'text']\n",
    "# Optional: only keep positive/negative labels if you want\n",
    "finance_df = df[df['label'] != 'neutral']\n",
    "\n",
    "# Get all words from the text\n",
    "all_words = \" \".join(finance_df['text'].tolist()).lower().split()\n",
    "\n",
    "# Count frequency\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Remove common stopwords\n",
    "stopwords = set([\"the\",\"a\",\"and\",\"of\",\"to\",\"in\",\"for\",\"on\",\"with\",\"as\",\"at\",\"is\",\"has\",\"that\", \".\", \",\", \"eur\", \"'s\", \"its'\", \"said\", \"(', ')\",\n",
    "                 \"it\", \"2009\", \"was\", \"2008\", \"2010\", \"2007\", \":\", \"its\", \"-\", \"``\", \"2006\"])\n",
    "finance_words = [word for word, count in word_counts.most_common(1000) if word not in stopwords]\n",
    "\n",
    "print(finance_words[:50])  # preview top words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legendary fund manager has surprising take on AI\n",
      "(1, array([[0.04693203, 0.8269804 , 0.12608758]], dtype=float32))\n",
      "Veteran analyst turns heads with new AMD stock target\n",
      "(1, array([[0.07726745, 0.48009095, 0.4426416 ]], dtype=float32))\n",
      "Legendary fund manager has surprising take on AI\n",
      "(1, array([[0.04693203, 0.8269804 , 0.12608758]], dtype=float32))\n",
      "Top Stock Movers Now: Nvidia, AMD, Dell, Fair Isaac, and More\n",
      "(1, array([[0.07136565, 0.611557  , 0.31707728]], dtype=float32))\n",
      "Nvidia CEO Jensen Huang Calls AMD's 10% OpenAI Stake Offer Surprising but Clever\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"f726ca01832b44599b281c99e7a3d0b8\"\n",
    "query = \"NVDA\"\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&language=en&sortBy=publishedAt&apiKey={API_KEY}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "articles = response.json()[\"articles\"]\n",
    "\n",
    "\n",
    "\n",
    "for a in articles[:5]:\n",
    "    print(a[\"title\"])\n",
    "    sentiment = prediction(a[\"title\"], model, vocab_dict)\n",
    "    print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a818433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyst Says Concerns About NVIDIA (NVDA) ‘Circle of CapEx Spending’ Are Not ‘Accurate’\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n",
      "Analyst Says Concerns About NVIDIA (NVDA) ‘Circle of CapEx Spending’ Are Not ‘Accurate’\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n",
      "Goldman Sachs strategist: No stock market bubble, yet\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n",
      "Bank of America Reiterates “Buy” on NVIDIA (NVDA), Calls It Top AI Pick\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n",
      "Intel Stock (INTC) Gets Price Target Lift on Strategic Moves and Foundry Expansion\n",
      "(1, array([[0.00897364, 0.9599625 , 0.03106392]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"f726ca01832b44599b281c99e7a3d0b8\"\n",
    "query = \"NVDA\"\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&language=en&sortBy=publishedAt&apiKey={API_KEY}\"\n",
    "\n",
    "response = requests.get(url).json()\n",
    "headlines = [article['title'] for article in response['articles']]\n",
    "\n",
    "# Keep only headlines with finance words\n",
    "filtered_headlines = [h for h in headlines if any(word in h.lower() for word in finance_words)]\n",
    "# more important words i want to include\n",
    "important_words = [\"profit\", \"loss\", \"earnings\", \"revenue\", \"merger\", \"acquisition\", \"downgrade\", \"lawsuit\", \"bankruptcy\", \"ipo\", \"interest\", \"rate\"]\n",
    "filtered_headlines = [h for h in filtered_headlines if any(word in h.lower() for word in important_words)]\n",
    "for line in filtered_headlines[:5]:\n",
    "    print(line)\n",
    "    sentiment = prediction(a[\"title\"], model, vocab_dict)\n",
    "    print(sentiment)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
